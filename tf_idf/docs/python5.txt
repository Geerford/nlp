You must have already heard about the occurrence of a new booming threat from China. And no, I am not talking about CoronaVirus.

No, you haven’t?
Source

These things above are called Click farms. And they are an industry already.

A single Click farm may contain thousands of iPhones and Android phones on shelves to influence and manipulate recommendation systems, propagate fake news, influencing public opinion, and sometimes even win elections — the need of the hour for some politicians in the USA.

    Fake news is everywhere. So are a lot of fake users.

At any time, millions of transactions are happening in our increasingly connected world. These transactions are time-bound, and we need to catch them pretty early before any harm is done dynamically.

So, can we catch these fraudulent transactions as they occur? Will Data Science come to rescue?

In this post, I am going to be talking about MIDAS, a Microcluster-Based Detector of Anomalies in Edge Streams, which aims to solve this exact problem.

PS: I wrote this post in collaboration with Siddharth Bhatia, who is the original author of MIDAS.
Data Used:

To work with the problem, we will use the DARPA Intrusion Detection dataset that is used in the MIDAS paper. DARPA has 4.5M IP →IP communications between 9.4K source IP and 2.3K destination IP over 87.7K minutes. Each communication is in the form of a directed edge (src_IP, dst_IP, timestamp, attack) where the ground truth attack label indicates whether the communication is an attack or not (anomalies are 60.1% of total). The data looks like below.

230.001.010.020,172.016.112.050,07/03/1998-18:17,neptune
230.001.010.020,172.016.112.050,07/03/1998-18:17,neptune
230.001.010.020,172.016.112.050,07/03/1998-18:17,neptune
230.001.010.020,172.016.112.050,07/03/1998-18:17,neptune
230.001.010.020,172.016.112.050,07/03/1998-18:17,neptune

But before usage in MIDAS, we have to change the src_node, dst_node, and timestamp to integers. Fortunately, the author of the library has done this preprocessing for us, and we can start with a preprocessed file from here. You can also download the original data here. The processed file looks like:

2,3,1
2,3,1
3,4,2
3,4,2
5,9,2
5,9,2

Algorithm Idea:
Count-Min-Sketch (CMS):

So first, let’s talk about CMS, which is the primary data structure used in this algorithm. From Wikipedia:

    In computing, the CMS is a probabilistic data structure that serves as a frequency table of events in a stream of data. It uses hash functions to map events to frequencies, but unlike a hash table uses only sub-linear space, at the expense of overcounting some events due to collisions.

Put simply, CMS is an approximate dictionary. A data structure that can keep approximate counts for the keys.

It is implemented as a grid with w columns and d rows with each of the d rows having a different hash function. Whenever we encounter a key, we add 1 to all the rows in the columns based on the hash function for the row. When we need to retrieve the value for the key, we go through all the rows and give the minimum value in all the rows for the given key.
Go through all rows j and get minimum value for the key. h is the hash function, and count is the grid name here.

The advantage of such a data structure is that we can get the value for a key in constant time as the size of the grid remains constant.
 Streaming Hypothesis Testing Approach(MIDAS):
The Uptick could be dangerous

Let’s start simple and talk about a single edge between two vertices u and v. We have the past trends of such edges, and we can see a considerable uptick at time period 10. We need to create an anomaly score that can capture such an uptick. In the real world, this uptick could be a DDOS attack. Or, it could be a scraping attempt from an IP address to another IP address.

So how can we capture such an uptick?

One approach would be to assume that the time series follows a particular generative model — for example, a Gaussian distribution. We could then find the mean and standard deviation of this Gaussian distribution and use these to declare the edges at a particular time tick as anomalous.

But that is a pretty strict condition. And the distribution could be anything. So, the MIDAS algorithm uses a weaker assumption that the mean level (i.e., the average rate at which edges appear) in the current time tick (e.g., t = 10) is the same as the mean level before the current time tick (t < 10).

So we set up a hypothesis test which avoids assuming any particular distribution for each time tick. We can divide the past edges into two classes:

    the current time tick (t = 10) and,
    all past time ticks (t < 10).

Now, let sᵤᵥ be the total number of edges from u to v up to the current time(t≤10). And aᵤᵥ be the number of edges from u to v at the current time(t=10).

We can keep these counts for all edges using two CMS data structures. One to hold the past hits and one to hold the current hits between two nodes.

So the number of hits at (t = 10) is aᵤᵥ, while the number of hits in past time ticks (t < 10) is sᵤᵥ − aᵤᵥ

Now, we can do the chi-squared goodness-of-fit test, which is used to determine whether sample data(t = 10) is consistent with a hypothesized distribution(t < 10). Our chi-squared statistic is:

The higher this statistic/anomaly score, the higher would be a chance of an anomalous edge.

The MIDAS Algorithm then simply looks like:
MIDAS-R

The above idea is extended in the MIDAS-R algorithm, which adds:

    Some time flexibility — Why consider a particular time tick and not the recent past? Edges in the recent past should also count toward the current time tick, but modified by reduced weight. A simple and efficient way to do this using our CMS data structures is as follows: At the end of every time tick, rather than resetting our CMS data structures aᵤᵥ, we reduce all its counts by a fixed fraction α ∈ (0, 1). This allows past edges to count toward the current time tick, with a diminished weight.
    Anomalous Node Scores — We would also want to catch large groups of spatially nearby edges: e.g., a single source IP address suddenly creating a large number of edges to many destinations or a small group of nodes suddenly creating an abnormally large number of edges between them. A simple intuition we use is that in either of these two cases, we expect to observe nodes with a sudden appearance of a large number of edges. Hence, we can use two additional CMS data structures to keep track of edge counts like before, except counting all edges adjacent to any node u. Specifically, we create CMS counters aᵤ and sᵤ to approximate the current and total edge counts adjacent to node u. Given each incoming edge (u, v), we can then compute three anomalousness scores: one for edge (u, v), as in our previous algorithm; one for node u, and one for node v. Finally, we combine the three scores by taking their maximum value.

This results in a more performant MIDAS-R algorithm.
Coding/Getting Started

We start by just getting the whole code:

git clone

We can then:

The output of the run command is as follows:

When we look at the scores.txt file we see:

We can then use the original DARPA file with the ground truth labels to check the AUC scores.
MIDAS-R achieves a much higher AUC (=0.95) compared to the baseline(=0.64) provided by SEDANSPOT(which uses personalized PageRank to detect anomalies in sublinear space and constant time per edge), while also running significantly faster (0.39s vs. 84s).

This is a 48% AUC improvement at 215 faster speed.
Conclusion

MIDAS provides a simple framework to find anomalies in any data which can be represented as a time-evolving/dynamic graph.

It can be used for a variety of use-cases in social networking as well as transactional sites to detect Microclusters and hence fraud.

So down with COVID-19 and down with the Fake news.