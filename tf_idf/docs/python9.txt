If you have assembled a standalone cluster and have enough knowledge about how Dask works in the background, it’s time for some practical Data Science. By the way, you can find all my source code and data in my GitHub account.
What are we trying to predict?

I want to classify the reviews of Amazon products into positive or negative, based on the text. Obviously, I need to do to some transformations to the data before I feed it to my ML training pipeline.
What about the data?

I’m using a published data set provided by Amazon with 3 million reviews and ratings. It is a medium-sized data set of 1.5 GB, but the purpose of this article is to show how and why you would use Dask to work on your much larger data sets.
Load the Cluster

Before going to our notebook we need a couple of quick commands to get our standalone cluster ready. If you encounter issues, this article will guide you to make a good standalone cluster.

First, establish an SSH connection with each of your machines. Let’s start with the machine you want to select as the Scheduler. You just need to input the following command:

dask-scheduler

My standalone Dask cluster has two workers (machines). If you want to connect these nodes with the Scheduler, you need to run the following command at the ‘Worker node’ terminal:
Scheduler Node

Now you have to connect the other machines as Worker nodes. To do this, you need to input the following commands at the Scheduler terminal. In the following example, the IP of the scheduler node is 10.1.1.93 and the port to which Dask should be connected is 8786.

# You should specify the IP and port of the scheduler node as below. dask-worker tcp://10.1.1.93:8786

Connect first worker to the scheduler node

I will connect four more machines as worker nodes, which makes my final cluster have a scheduler node and five worker nodes. After you connect each of your nodes, you should see something like this in your Scheduler terminal:
Scheduler Virtual machine after connect five worker nodes

Congrats! You’re now ready to move to your Jupyter Notebook.

Dask provides a nice dashboard where you can see all the metrics of the processes that are running. You can get it running using the following commands:
Click the dashboard link, and it should open a new tab at your browser. Dask’s dashboard allows you to see all the computations that each worker is doing in real time. Awesome, right?
Dask Dataframes

Load the data into a Dask Dataframe by following the example below.

let’s see the data frame generated by Dask :
Dask DataFrame

Wow! It looks quite different from a pandas Dataframe, right? This is because of two main reasons:

    Dask’s read_csv function divides the data into small pandas partitions, allowing us to handle big data sets. In our case, Dask divided 1.5 GB dataset into 22 partitions, making us have partitions of 68.18 MB. As a general advice, when you work with larger data sets - your chunks of data should be small enough so that most of them fit in a worker’s available memory at once. You can control this when you select partition size in Dask DataFrame or chunk size in Dask Array.
    Dask uses lazy computations like Spark. Dask is a graph execution engine, so all the different tasks are delayed, which means that no functions are actually executed until you hit the function .compute(). In the above example, we have 66 delayed tasks.

Let’s hit .compute() and see what happens.

The .compute function assembled 22 small partitions into one pandas DataFrame that contains 3 million reviews. Don’t do this if your data set is giant, because doing this makes you bring the data set into local memory on a single machine, which is what Dask is trying to avoid.
Missing Values

Checking for missing values is similar to how you would do a .isnull() function on a pandas DataFrame. But remember, you have to hit .compute() if you want to see the results.
missing values

Good news, there are no missing values to deal with in our example dataset.
Descriptive Statistics

Let’s have a brief look at the first five rows of the data in table, so we can see what kinds of values we’re working with. Dask dataframes allow you to use pandas functions like .head(), .groupby(), .loc(), .merge() and so on.
Dataframe

We have three columns: title, review and rating. I would like to know if we are working with an imbalance in the data, so I will count the values of my target variable (rating).

This looks a very good ‘train’ data set because the classes are represented equally. We don’t need to be worried about applying techniques to resolve imbalanced data.

You can definitely do more sophisticated EDA for text analyses, like finding the most frequent terms, LDA topic detection, clustering reviews based on products and so on, to add new features to your train data set. However, these topics are out of scope for this article.

Coming this far, I think that you are getting the point - Dask is a friendly library for Python users, allowing you to work with BIG DATA .
Data wrangling

For example, if we want to set up a binary classification problem, we need to classify the reviews into “Positive” or “Negative”. Amazon users rank the reviews among 1–5 stars. To convert this, I assumed to classify ratings ranging from 1 to 3 stars as Negative and labeled them with value 0, and vice versa.

Keep in mind - Dask converts your scenario in such a way that you are working with small chunks of pandas DataFrames even if your data is huge, so this is the most elegant and efficient way to iterate over all the partitions.

Let’s take a look at the DataFrame “df2”:
df2

We have created a DAG with 177 tasks. Remember, we won’t do any computation until we hit .compute().

Let’s see the output of our transformations.
New target variable

We’ve successfully converted the problem into a binary text classification.

I also combined the “title” and the “review” features before cleaning noise from the text. According to this scenario, it is evident that most important data points exist in the reviews but sometimes, the important data points also exist in the title. I want to make sure I consider both.
Managing Memory

Before continuing, it is important to know how to manage RAM usage with Dask.

dask.distributed stores the results of tasks in the distributed memory of the worker nodes. The central scheduler node tracks all data on the cluster and determines when data should be freed. Completed results are usually cleared from memory as quickly as possible in order to make room for more computation. However, If you want to store the dataframe in RAM, you should use the function persist.

What it is the difference between compute and persist?

compute returns a single output per input, while persist returns a copy of the collection with each block (or partition), replaced by a single output. In short, use persist to keep a full collection on the cluster and use compute when you want a small result as a discrete output.

But you may be wondering why would I want to use persist ?

Accessing data from RAM is often much faster than accessing it from disk. Once you have your data set in a clean state that both:

    Fits in memory, AND
    Is clean enough that you will want to try many different analyses,

Then it is a good time to persist your data in RAM.

Let’s see an example :
df3 generated by the previous section

Let’s persist df3...
persist df

So basically persist() returns a copy for each of the Dask tasks with their previous lazy computations, now submitted to run on the cluster. Otherwise you have the option to save your dataframe to local (not recommended) or to your cluster to parquet format.

Saving df3 to parquet:
Clean Text for Machine Learning

When we vectorize the text, we’ll convert our corpus(reviews) into what is called a ‘sparse matrix’. A sparse matrix is a matrix comprised of mostly zero values. The sparsity of a matrix can be quantified with a score, which is the number of zero values in the matrix divided by the total number of elements in the matrix. It may cause problems with regards to space and time complexity. Therefore, cleaning noise from the reviews becomes crucial to improve the performance of the training and the outcome model. I’ve attached a portion of my cleaning algorithm but you will find even more pieces at my GitHub.
cleaning Data

But we need one more step - we need to apply the clean functions to all the partitions. To do this, use map_partitons() function provided by Dask, instead of map() from pandas.

When you call map_partitions (just like when you call .apply() on pandas.DataFrame), the function that you try to map (or apply) will take dataframe as a first argument.

In case of dask.dataframe.map_partitions this first argument will be a partition and in case of pandas.DataFrame.apply, it will be a whole dataframe. This means that your function must accept a dataframe partition as a first argument, and your code could look like this:
map_partitions

Let’s check the results!
cleaned df

Nice!! We have cleaned our text with Dask! Thanks a lot for reading.